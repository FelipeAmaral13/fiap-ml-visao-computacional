{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando deep-learning para solução de captcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versão local deste notebook, sem o sufixo ```-collab```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requerimentos\n",
    "\n",
    "Todas as bibliotecas já estão instaladas no Google Colab.\n",
    "\n",
    "* OpenCV>=4.0.0\n",
    "* Keras>=2.3.1\n",
    "* Matplotlib>=3.1.3\n",
    "* Seaborn>=0.0.10\n",
    "* Scikit Learn>=0.22.1\n",
    "* Imutils>=0.5.3\n",
    "\n",
    "### 1.2 Arquivos\n",
    "\n",
    "Baixe o repositório do GitHub utilizando o comando abaixo. Em caso de atualização, utilize o comando para apagar o diretório antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fiap-ml-visao-computacional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michelpf/fiap-ml-visao-computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora posicionar o diretório do repositório para a aula respectiva. Nesse caso envie o comando de mudança de diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fiap-ml-visao-computacional/aula-5-machine-learning-aplicado/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas que serão utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from imutils import paths\n",
    "import imutils\n",
    "\n",
    "#Exibição na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um modelo de Deep Learning para reconhecimento de caracteres, vamos utilizar a base conhecida gerada de um sistema de captcha que foi utilizado nos sistemas do Tribunal Regional do Trabalho de São Paulo.\n",
    "\n",
    "Neste exemplo, vamos utilizar somente os caracteres que aparecem na maior parte dos desafios de captcha, que na coleta foram de 33 letras e números.\n",
    "\n",
    "A arquitetura e alguns componentes foram adaptados deste [artigo](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d) de Orhan Gazi Yalçın."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Identificando as classes\n",
    "\n",
    "Como um gerador de captchas nem sempre explora todo o alfabeto, vamos identificar exatamente quais as letras são utilizadas para listar todas as possíveis classes deste problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_imagens_treino = \"captcha/imagens/\"\n",
    "\n",
    "lista_classes = []\n",
    "\n",
    "for file in glob.glob(os.path.join(\"captcha/imagens/\", \"*\")):\n",
    "    lista_classes.append(file.split(\"/\")[-1])\n",
    "\n",
    "lista_classes = list(set(lista_classes))\n",
    "print(lista_classes)\n",
    "print(len(lista_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Enquadramento de imagem\n",
    "\n",
    "Vamos deixar uma borda de segurança entre as letras para evitar classificações indevidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimensionar_borda(imagem, comprimento, altura):\n",
    "\n",
    "    # Obtendo as dimensões da imagem\n",
    "    (h, w) = imagem.shape[:2]\n",
    "\n",
    "    # Vamos deixar as imagens quadradas, logo se o comprimento for maior que a altura\n",
    "    if w > h:\n",
    "        imagem = imutils.resize(imagem, width=comprimento)\n",
    "    else:\n",
    "        imagem = imutils.resize(imagem, height=altura)\n",
    "\n",
    "    # Ajustando a borda\n",
    "    padW = int((comprimento - imagem.shape[1]) / 2)\n",
    "    padH = int((altura - imagem.shape[0]) / 2)\n",
    "\n",
    "    imagem = cv2.copyMakeBorder(imagem, padH, padH, padW, padW, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
    "    imagem = cv2.resize(imagem, (comprimento, altura), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de imagem com tamanho despadronizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"captcha/imagens/i/000001_3ibaz.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_sem_borda = cv2.resize(imagem, (20, 20), interpolation=cv2.INTER_LANCZOS4)\n",
    "plt.imshow(imagem_sem_borda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de imagem padronizada, com margem de segurança e tamanho único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_padronizada = redimensionar_borda(imagem, 20, 20)\n",
    "plt.imshow(imagem_padronizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Treinamento\n",
    "\n",
    "Colecionando imagens para treinamento e realizando pequenos ajustes para posterior uso na biblioteca de deep-learning do Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "dados_imagem = []\n",
    "\n",
    "for imagem_caminho in paths.list_images(pasta_imagens_treino):\n",
    "\n",
    "    # Obtendo imagem e convertendo para escala de cinza\n",
    "    imagem = cv2.imread(imagem_caminho)\n",
    "    imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    imagem = redimensionar_borda(imagem, 20, 20)\n",
    "        \n",
    "    # Adicionando uma terceira dimensão (Canal Normalizado) conforme especificação do Keras\n",
    "    imagem = np.expand_dims(imagem, axis=2)\n",
    "\n",
    "    # Obtendo a caractere pelo nome do diretório\n",
    "    classe = imagem_caminho.split(os.path.sep)[-2]\n",
    "    \n",
    "    dados_imagem.append(imagem)\n",
    "    classes.append(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao todo temos as seguintes quantidades de exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes), len(dados_imagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também simplificar a informação de escala de cinza. Utilizaremos a forma normalizada, dividindo todos os valores por 255. Desta forma um pixel 100% branco seria 1, e outro 100% preto seria 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_imagem = np.array(dados_imagem, dtype=\"float\") / 255\n",
    "classes = np.array(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a divisão de treinamento e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, y_train, y_test) = train_test_split(dados_imagem, classes, test_size=0.3, \n",
    "                                                      random_state=0)\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras trabalha com uma forma diferente dos dados. Ao invés de utilizar as 3 dimensões, precisaremos de mais uma dimensão para incluir as imagens que farão parte dos treinamentos e testes, obtendo **Número de Imagens, Comprimento, Largura, Canal Normalizado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato de dados da API Keras\", x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Imagens de treino (x) 20 x 20:', x_train.shape)\n",
    "print('Quantidade de imagens de treino', x_train.shape[0])\n",
    "print('Quantidade de imagens de treino', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a entrada dos dados, neste caso precisa ser exatamente da mesma forma que as imagens forem treinadas. Isso é portante pois a rede neural estará preparada para inferir somente imagens com este tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_entrada = (20, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluindo codificação _one-hot_, ou seja, um conjunto de dados que está associado as classes, logo um dos 33 caracteres será codificado com bit 1 de acordo com sua posição na lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer().fit(y_train)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de uma amostra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lista_classes)\n",
    "print([lista_classes[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vamos constuir um modelo simples, do zero. Como as imagens são bem simples, diversas arquiteturas funcionam.\n",
    "\n",
    "Quando lidamos com objetos mais complexos, é bem comum optarmos por arquiteturas abertas como por exemplo:\n",
    "\n",
    "*[VGG](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) de Oxford\n",
    "*[ResNet](https://arxiv.org/abs/1512.03385) da Microsoft\n",
    "*[Inception](https://github.com/google/inception) do Google\n",
    "*[Xception](https://arxiv.org/abs/1610.02357) do Google\n",
    "\n",
    "Depois de avaliar estas arquiteturas, é possível adapta-las para classificar imagens específicas, isso se dá alterando as últimas camadas. É o que chamamos também de **Transfer Learning**.\n",
    "\n",
    "Neste [link](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) você pode encontrar mais sobre outras arquiteturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_classes = len(lista_classes)\n",
    "numero_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Arquitetura\n",
    "\n",
    "Partimos de um modelo simples que na maioria das vezes resolve problemas de OCR como esse. Como foi citado, identificações mais complexas utilizamos outras aboragens ou evolução de uma arquitetura inicial como esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Este componente, se trata de um filtro ou uma camada convulacional. Ela será responsável por \n",
    "# colocar uma janela de kernel (5x5), navegar pela imagem e extrair a soma dos pixels de cada janela\n",
    "# o passo para mover a janela, chamado Stride, por padrão é de um pixel\n",
    "model.add(Conv2D(20, kernel_size=(5,5), padding=\"same\", input_shape=shape_entrada, activation=\"relu\"))\n",
    "\n",
    "# A camada de Pooling (ou MaxPooling2D) tem o papel de reduzir a dimensionalidade. Neste caso, a partir \n",
    "# da etapa anterior, será dividia em grupos de 2 x 2 pixels e será obtida o maior valor deles\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "# Esta etapa conhecida como \"achatamento\" é onde abrimos os dados organizados em tabelas (ou matrizes) \n",
    "# para uma única linha\n",
    "model.add(Flatten())\n",
    "\n",
    "# A camada densa (ou Dense) conectará cada elemento da camada anterior e passará para a próxima\n",
    "# camada com as classes existentes\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# O Dropout é um ruído gerado para evitar overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# A camada final, determinará qual classe escolher. Por tal razão ela possui a ativação Softmax, que retorna \n",
    "# a probabilidade por classe\n",
    "model.add(Dense(numero_classes, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para revisão de funções de ativação, em particular [Relu](https://matheusfacure.github.io/2017/07/12/activ-func/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=10, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os gráficos a seguir mostram convergência de acurácia para os dados de treinamento e validação.\n",
    "Note que o valor do erro, diferentemente da acurácia, não é expressada em porcentagem, portanto erro < 1 é um ótimo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para deixar no formato do Seaborn os gráficos do Pyplot\n",
    "sns.set()\n",
    "\n",
    "# Exibindo dados de Acurácia/Precisão\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Exibindo dados de Perda\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo para uso posterior. Mesmo imagens pequenas como essas levam vários minutos para treinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo no formato HDf5\n",
    "model.save(\"modelos/model_captcha.h5\")\n",
    "\n",
    "# Arquitetura das camadas em JSSON e pesos treinados em HDF5\n",
    "model.save_weights(\"pesos/weights_captcha.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez salvo o modelo, nesta etapa é só carregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo no formato HDf5\n",
    "model = load_model(\"modelos/model_captcha.h5\")  \n",
    "model.load_weights(\"pesos/weights_captcha.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inferir algumas imagens para verificar visualmente como o classificador está se comportando.\n",
    "Para isso definimos uma função para normalizar uma imagem do captcha, para extrair os ruídos e posteriomente cada uma das suas letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagem_normalizada(caminho_imagem):\n",
    "\n",
    "    imagem = cv2.imread(caminho_imagem)\n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    imagem_suavizada = cv2.GaussianBlur(imagem_gray, (5, 5), 0)\n",
    "    _, imagem_limiarizada =  cv2.threshold(imagem_suavizada, 148, 255, cv2.THRESH_BINARY_INV)\n",
    "    _, imagem_limiarizada =  cv2.threshold(imagem_limiarizada, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    imagem_erodida = cv2.erode(imagem_limiarizada, kernel, iterations = 2)\n",
    "\n",
    "    return imagem_erodida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As etapas da função são as seguintes:\n",
    "\n",
    "### 3.1 Conversão para escala de cinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "imagem_caminho = \"captcha/anotados/d85iq.png\"\n",
    "\n",
    "imagem_original = cv2.imread(imagem_caminho, cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(imagem_original, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Suavização para preparação de limiarização\n",
    "\n",
    "Esta operação visa remover os ruídos da imagem, como as linhas transversais e pequenos pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(imagem_caminho)\n",
    "imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "imagem_suavizada = cv2.GaussianBlur(imagem_gray, (5, 5), 0)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_suavizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Limiarização\n",
    "\n",
    "A limirização remove todos os ruídos baseado num valor de limiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, imagem_limiarizada =  cv2.threshold(imagem_suavizada, 148, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_limiarizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Inversão da imagem\n",
    "\n",
    "Para se adequar as imagens de treinamento e para que fique mais nítido a visualização das letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, imagem_limiarizada =  cv2.threshold(imagem_limiarizada, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_limiarizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Erosão\n",
    "\n",
    "Como a imagem está invertida, aplicamos uma erosão para intensificar as linhas e melhorar a nitidez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "imagem_erodida = cv2.erode(imagem_limiarizada, kernel, iterations = 2)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_erodida, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução da função. Neste caso não fizemos a inversão da imagem pois as letras foram treinadas com o fundo branco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_norm = imagem_normalizada(imagem_caminho)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_norm, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Identificação de contornos\n",
    "\n",
    "Com a imagem com as letras bem definidas, iremos aplicar o método Canny para extrair os contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_borda = cv2.Canny(imagem_norm, 30, 200)\n",
    "contornos, _ = cv2.findContours(imagem_borda, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_borda, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_borda_contornos = imagem_borda.copy()\n",
    "imagem_borda_contornos = cv2.cvtColor(imagem_borda_contornos, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "cv2.drawContours(imagem_borda_contornos, contornos, -1, (0,255,0), 1)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_borda_contornos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contornos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Extração das letras\n",
    "\n",
    "A função a seguir, analisará os contornos identificados e fará um filtro baseado no tamanho do contorno. Em algumas ocasiões é possível ter contornos identificados em pequenos ruídos que ainda passam pelo processo, mas como eles são pequenos são facilmente identificados e removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_letras(imagem):\n",
    "    \n",
    "    contornos_letras = []\n",
    "    \n",
    "    imagem_borda = cv2.Canny(imagem, 30, 200)\n",
    "    contornos, _ = cv2.findContours(imagem_borda, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contorno in contornos:\n",
    "        (x, y, w, h) = cv2.boundingRect(contorno)\n",
    "\n",
    "        area = int(w) * int(h)\n",
    "        \n",
    "        if area <250:\n",
    "            continue\n",
    "        \n",
    "        contornos_letras.append((x, y, w, h))\n",
    "    \n",
    "    print(\"Identificado \" + str(len(contornos_letras)) + \" contornos válidos.\")\n",
    "    \n",
    "    # Se detectar mais do que 5 letras, detecção inválida\n",
    "    if len(contornos_letras) < 5 :\n",
    "        return False\n",
    "    \n",
    "    contornos_letras = sorted(contornos_letras, key=lambda x: x[0])\n",
    "    \n",
    "    lista_imagem_letras = []\n",
    "    \n",
    "    for retangulo_letra in contornos_letras:\n",
    "        x, y, w, h = retangulo_letra\n",
    "        imagem_letra = imagem[y - 10:y + h + 30, x - 1:x + w + 1]\n",
    "        lista_imagem_letras.append(imagem_letra)\n",
    "        \n",
    "    return lista_imagem_letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O retorno da função é a lista de regiões de interesse das letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_letras = obter_letras(imagem_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Validação com imagem completa\n",
    "\n",
    "A função a seguir, dada uma imagem, vai padronizá-la e inferir letra a letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_predicao(imagem):\n",
    "    \n",
    "    imagem_norm = redimensionar_borda(imagem, 20, 20)\n",
    "    prediction = model.predict(imagem_norm.reshape(1, 20, 20, 1))\n",
    "    label = lb.inverse_transform(prediction)[0]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(151)\n",
    "plt.title(obter_predicao(imagem_letras[0]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[0], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.title(obter_predicao(imagem_letras[1]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[1], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.title(obter_predicao(imagem_letras[2]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[2], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.title(obter_predicao(imagem_letras[3]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[3], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.title(obter_predicao(imagem_letras[4]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[4], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
