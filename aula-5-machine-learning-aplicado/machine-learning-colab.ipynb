{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rastreamento de objetos e machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rastreamento de cores, objetos e aplicação de técnicas de machine learning pré-treinadas e desenvolvimento de algoritmos próprios.\n",
    "\n",
    "Alguns recursos e códigos foram adaptados deste [repositório](https://github.com/udacity/CVND_Exercises/) do curso de Visão Computacional da Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versão local deste notebook, sem o sufixo ```-collab```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requerimentos\n",
    "\n",
    "### 1.1 Bibliotecas\n",
    "\n",
    "* OpenCV>=3.4.3\n",
    "* Pillow>= 7.0.0\n",
    "* Pytorch>=1.4.0\n",
    "* Numpy>=1.18.1\n",
    "\n",
    "### 1.2 Arquivos\n",
    "\n",
    "Baixe o repositório do GitHub utilizando o comando abaixo. Em caso de atualização, utilize o comando para apagar o diretório antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fiap-ml-visao-computacional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michelpf/fiap-ml-visao-computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora posicionar o diretório do repositório para a aula respectiva. Nesse caso envie o comando de mudança de diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fiap-ml-visao-computacional/aula-5-machine-learning-aplicado/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#Exibição na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL\n",
    "\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reconhecimento Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando 100 exemplos de faces, utilizando a câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrator de faces\n",
    "def face_extractor(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "    imagem_gray = cv2.cvtColor(imagem,cv2.COLOR_RGB2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        face_recortada = imagem[y:y+h, x:x+w]\n",
    "\n",
    "    return face_recortada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extração de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando imagems de exemplos para ser posteriormente treinado. Neste caso vamos adotar um tamanho de imagem para processamento de 200 x 200 (empírico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/camera_output_1.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"Extração de faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o diretório ```imagens/faces/michel``` que contem imagens de treino de uma pessoa (eu 😁)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento do modelo\n",
    "\n",
    "Podemos testar os diversos tipos de classificadores, no entanto, o classificador LBPH tem o uso com melhor performance dentre o Eingenfaces e Fisherfaces.\n",
    "Neste caso como temos apenas uma única pessoa, nosso dicionário de pessoas, ficou apenas com um único registro. Em casos de multiclasses, ou seja, mais de uma pessoa, cada uma delas deve ter um *id* associado, que é o valor de chave do dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando exemplos de arquivos previamente coletados\n",
    "faces_path = 'imagens/faces/michel/'\n",
    "lista_arquivos_imagens = [f for f in listdir(faces_path) if isfile(join(faces_path, f))]\n",
    "\n",
    "dados_treinamento, labels = [], []\n",
    "\n",
    "# Lendo as imagens e associando a um label\n",
    "for i, arquivos in enumerate(lista_arquivos_imagens):\n",
    "    imagem_path = faces_path + lista_arquivos_imagens[i]\n",
    "    imagem = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "    dados_treinamento.append(imagem)\n",
    "    labels.append(0)\n",
    "\n",
    "# Criando uma matriz da lista de labels\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "model.train(dados_treinamento, labels)\n",
    "\n",
    "print(\"Modelo treinado com sucesso.\")\n",
    "\n",
    "pessoas = {0: \"Michel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferência do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para identificar o rosto e segmentar da imagme principal. Também utilizaremos para desenhar um retângulo delimitador.\n",
    "Note que estamos normalizando a imagem (mesma escala, 200 x 200) que as imagens de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def face_detector(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "    imagem_gray = cv2.cvtColor(imagem,cv2.COLOR_RGB2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.1, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return imagem, [], 0, 0\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(imagem,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    \n",
    "    return imagem, roi, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo as imagems por meio da câmera e fazendo a inferência on-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/analise-face-michel.PNG\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "imagem, face, x, y = face_detector(imagem)\n",
    "\n",
    "if face is not ():\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)\n",
    "    predicao = model.predict(face)\n",
    "\n",
    "    if x > 0:\n",
    "        notificacao = \"Dist. \" + str(int(predicao[1])) + ' ' + pessoas[predicao[0]] \n",
    "        cv2.putText(imagem, notificacao, (x, y-20), cv2.FONT_HERSHEY_DUPLEX, 1, (255,120,150), 2)\n",
    "\n",
    "    if int(predicao[1]) < 40:\n",
    "        cv2.putText(imagem, \"Reconhecido com sucesso\", (x, y-50), cv2.FONT_HERSHEY_DUPLEX, 1, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(imagem, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"Inferência do modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classificador de Objetos\n",
    "#### Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário baixar os pesos (modelo de deep-learning) neste link https://pjreddie.com/media/files/yolov3.weights e copiar para  pasta weights. O comando a seguir vai baixar o arquivo de pesos no diretório ```pesos```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights -P pesos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "# Configurações na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'pesos/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# Rótulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topologia da rede neural da YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho da imagem de entrada da rede: \" + str(m.width) + \"x\" + str(m.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da figura\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregando imagem para classificar\n",
    "img_path = \"imagens/camara.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Convertendo para o espaço de cores RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatível com a primeira camada da rede neural  \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Exibição das imagens\n",
    "plt.subplot(121)\n",
    "plt.title(\"Imagem Original\")\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Imagem Redimensionada\")\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retângulo \n",
    "# de identificação de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do gráfico\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregar imagem para classificação\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Conversão para o espaço RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionamento para adatapção da primeira camada da rede neural \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Deteteção de objetos na imagem\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Objetos encontrados e nível de confiança\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "# Desenho no gráfico com os regângulos e rótulos\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(boxes, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
